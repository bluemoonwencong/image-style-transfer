{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "\n",
    "def vgg_arg_scope(weight_decay=0.0005):\n",
    "  \"\"\"Defines the VGG arg scope.\n",
    "  Args:\n",
    "    weight_decay: The l2 regularization coefficient.\n",
    "  Returns:\n",
    "    An arg_scope.\n",
    "  \"\"\"\n",
    "  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n",
    "                      activation_fn=tf.nn.relu,\n",
    "                      weights_regularizer=slim.l2_regularizer(weight_decay),\n",
    "                      biases_initializer=tf.zeros_initializer()):\n",
    "    with slim.arg_scope([slim.conv2d], padding='SAME') as arg_sc:\n",
    "      return arg_sc\n",
    "\n",
    "\n",
    "def vgg_a(inputs,\n",
    "          num_classes=1000,\n",
    "          is_training=True,\n",
    "          dropout_keep_prob=0.5,\n",
    "          spatial_squeeze=True,\n",
    "          scope='vgg_a',\n",
    "          fc_conv_padding='VALID',\n",
    "          global_pool=False):\n",
    "  \"\"\"Oxford Net VGG 11-Layers version A Example.\n",
    "  Note: All the fully_connected layers have been transformed to conv2d layers.\n",
    "        To use in classification mode, resize input to 224x224.\n",
    "  Args:\n",
    "    inputs: a tensor of size [batch_size, height, width, channels].\n",
    "    num_classes: number of predicted classes. If 0 or None, the logits layer is\n",
    "      omitted and the input features to the logits layer are returned instead.\n",
    "    is_training: whether or not the model is being trained.\n",
    "    dropout_keep_prob: the probability that activations are kept in the dropout\n",
    "      layers during training.\n",
    "    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n",
    "      outputs. Useful to remove unnecessary dimensions for classification.\n",
    "    scope: Optional scope for the variables.\n",
    "    fc_conv_padding: the type of padding to use for the fully connected layer\n",
    "      that is implemented as a convolutional layer. Use 'SAME' padding if you\n",
    "      are applying the network in a fully convolutional manner and want to\n",
    "      get a prediction map downsampled by a factor of 32 as an output.\n",
    "      Otherwise, the output prediction map will be (input / 32) - 6 in case of\n",
    "      'VALID' padding.\n",
    "    global_pool: Optional boolean flag. If True, the input to the classification\n",
    "      layer is avgpooled to size 1x1, for any input size. (This is not part\n",
    "      of the original VGG architecture.)\n",
    "  Returns:\n",
    "    net: the output of the logits layer (if num_classes is a non-zero integer),\n",
    "      or the input to the logits layer (if num_classes is 0 or None).\n",
    "    end_points: a dict of tensors with intermediate activations.\n",
    "  \"\"\"\n",
    "  with tf.variable_scope(scope, 'vgg_a', [inputs]) as sc:\n",
    "    end_points_collection = sc.original_name_scope + '_end_points'\n",
    "    # Collect outputs for conv2d, fully_connected and max_pool2d.\n",
    "    with slim.arg_scope([slim.conv2d, slim.max_pool2d],\n",
    "                        outputs_collections=end_points_collection):\n",
    "      net = slim.repeat(inputs, 1, slim.conv2d, 64, [3, 3], scope='conv1')\n",
    "      net = slim.max_pool2d(net, [2, 2], scope='pool1')\n",
    "      net = slim.repeat(net, 1, slim.conv2d, 128, [3, 3], scope='conv2')\n",
    "      net = slim.max_pool2d(net, [2, 2], scope='pool2')\n",
    "      net = slim.repeat(net, 2, slim.conv2d, 256, [3, 3], scope='conv3')\n",
    "      net = slim.max_pool2d(net, [2, 2], scope='pool3')\n",
    "      net = slim.repeat(net, 2, slim.conv2d, 512, [3, 3], scope='conv4')\n",
    "      net = slim.max_pool2d(net, [2, 2], scope='pool4')\n",
    "      net = slim.repeat(net, 2, slim.conv2d, 512, [3, 3], scope='conv5')\n",
    "      net = slim.max_pool2d(net, [2, 2], scope='pool5')\n",
    "\n",
    "      # Use conv2d instead of fully_connected layers.\n",
    "      net = slim.conv2d(net, 4096, [7, 7], padding=fc_conv_padding, scope='fc6')\n",
    "      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n",
    "                         scope='dropout6')\n",
    "      net = slim.conv2d(net, 4096, [1, 1], scope='fc7')\n",
    "      # Convert end_points_collection into a end_point dict.\n",
    "      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n",
    "      if global_pool:\n",
    "        net = tf.reduce_mean(net, [1, 2], keep_dims=True, name='global_pool')\n",
    "        end_points['global_pool'] = net\n",
    "      if num_classes:\n",
    "        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n",
    "                           scope='dropout7')\n",
    "        net = slim.conv2d(net, num_classes, [1, 1],\n",
    "                          activation_fn=None,\n",
    "                          normalizer_fn=None,\n",
    "                          scope='fc8')\n",
    "        if spatial_squeeze:\n",
    "          net = tf.squeeze(net, [1, 2], name='fc8/squeezed')\n",
    "        end_points[sc.name + '/fc8'] = net\n",
    "      return net, end_points\n",
    "vgg_a.default_image_size = 224\n",
    "\n",
    "\n",
    "def vgg_16(inputs,\n",
    "           num_classes=1000,\n",
    "           is_training=True,\n",
    "           dropout_keep_prob=0.5,\n",
    "           spatial_squeeze=True,\n",
    "           scope='vgg_16',\n",
    "           fc_conv_padding='VALID',\n",
    "           global_pool=False):\n",
    "  \"\"\"Oxford Net VGG 16-Layers version D Example.\n",
    "  Note: All the fully_connected layers have been transformed to conv2d layers.\n",
    "        To use in classification mode, resize input to 224x224.\n",
    "  Args:\n",
    "    inputs: a tensor of size [batch_size, height, width, channels].\n",
    "    num_classes: number of predicted classes. If 0 or None, the logits layer is\n",
    "      omitted and the input features to the logits layer are returned instead.\n",
    "    is_training: whether or not the model is being trained.\n",
    "    dropout_keep_prob: the probability that activations are kept in the dropout\n",
    "      layers during training.\n",
    "    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n",
    "      outputs. Useful to remove unnecessary dimensions for classification.\n",
    "    scope: Optional scope for the variables.\n",
    "    fc_conv_padding: the type of padding to use for the fully connected layer\n",
    "      that is implemented as a convolutional layer. Use 'SAME' padding if you\n",
    "      are applying the network in a fully convolutional manner and want to\n",
    "      get a prediction map downsampled by a factor of 32 as an output.\n",
    "      Otherwise, the output prediction map will be (input / 32) - 6 in case of\n",
    "      'VALID' padding.\n",
    "    global_pool: Optional boolean flag. If True, the input to the classification\n",
    "      layer is avgpooled to size 1x1, for any input size. (This is not part\n",
    "      of the original VGG architecture.)\n",
    "  Returns:\n",
    "    net: the output of the logits layer (if num_classes is a non-zero integer),\n",
    "      or the input to the logits layer (if num_classes is 0 or None).\n",
    "    end_points: a dict of tensors with intermediate activations.\n",
    "  \"\"\"\n",
    "  with tf.variable_scope(scope, 'vgg_16', [inputs]) as sc:\n",
    "    end_points_collection = sc.original_name_scope + '_end_points'\n",
    "    # Collect outputs for conv2d, fully_connected and max_pool2d.\n",
    "    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n",
    "                        outputs_collections=end_points_collection):\n",
    "      net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope='conv1')\n",
    "      net = slim.max_pool2d(net, [2, 2], scope='pool1')\n",
    "      net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope='conv2')\n",
    "      net = slim.max_pool2d(net, [2, 2], scope='pool2')\n",
    "      net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3], scope='conv3')\n",
    "      net = slim.max_pool2d(net, [2, 2], scope='pool3')\n",
    "      net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv4')\n",
    "      net = slim.max_pool2d(net, [2, 2], scope='pool4')\n",
    "      net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv5')\n",
    "      net = slim.max_pool2d(net, [2, 2], scope='pool5')\n",
    "\n",
    "      # Use conv2d instead of fully_connected layers.\n",
    "      net = slim.conv2d(net, 4096, [7, 7], padding=fc_conv_padding, scope='fc6')\n",
    "      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n",
    "                         scope='dropout6')\n",
    "      net = slim.conv2d(net, 4096, [1, 1], scope='fc7')\n",
    "      # Convert end_points_collection into a end_point dict.\n",
    "      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n",
    "      if global_pool:\n",
    "        net = tf.reduce_mean(net, [1, 2], keep_dims=True, name='global_pool')\n",
    "        end_points['global_pool'] = net\n",
    "      if num_classes:\n",
    "        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n",
    "                           scope='dropout7')\n",
    "        net = slim.conv2d(net, num_classes, [1, 1],\n",
    "                          activation_fn=None,\n",
    "                          normalizer_fn=None,\n",
    "                          scope='fc8')\n",
    "        if spatial_squeeze:\n",
    "          net = tf.squeeze(net, [1, 2], name='fc8/squeezed')\n",
    "        end_points[sc.name + '/fc8'] = net\n",
    "      return net, end_points\n",
    "vgg_16.default_image_size = 224\n",
    "\n",
    "\n",
    "def vgg_19(inputs,\n",
    "           num_classes=1000,\n",
    "           is_training=True,\n",
    "           dropout_keep_prob=0.5,\n",
    "           spatial_squeeze=True,\n",
    "           scope='vgg_19',\n",
    "           fc_conv_padding='VALID',\n",
    "           global_pool=False):\n",
    "  \"\"\"Oxford Net VGG 19-Layers version E Example.\n",
    "  Note: All the fully_connected layers have been transformed to conv2d layers.\n",
    "        To use in classification mode, resize input to 224x224.\n",
    "  Args:\n",
    "    inputs: a tensor of size [batch_size, height, width, channels].\n",
    "    num_classes: number of predicted classes. If 0 or None, the logits layer is\n",
    "      omitted and the input features to the logits layer are returned instead.\n",
    "    is_training: whether or not the model is being trained.\n",
    "    dropout_keep_prob: the probability that activations are kept in the dropout\n",
    "      layers during training.\n",
    "    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n",
    "      outputs. Useful to remove unnecessary dimensions for classification.\n",
    "    scope: Optional scope for the variables.\n",
    "    fc_conv_padding: the type of padding to use for the fully connected layer\n",
    "      that is implemented as a convolutional layer. Use 'SAME' padding if you\n",
    "      are applying the network in a fully convolutional manner and want to\n",
    "      get a prediction map downsampled by a factor of 32 as an output.\n",
    "      Otherwise, the output prediction map will be (input / 32) - 6 in case of\n",
    "      'VALID' padding.\n",
    "    global_pool: Optional boolean flag. If True, the input to the classification\n",
    "      layer is avgpooled to size 1x1, for any input size. (This is not part\n",
    "      of the original VGG architecture.)\n",
    "  Returns:\n",
    "    net: the output of the logits layer (if num_classes is a non-zero integer),\n",
    "      or the non-dropped-out input to the logits layer (if num_classes is 0 or\n",
    "      None).\n",
    "    end_points: a dict of tensors with intermediate activations.\n",
    "  \"\"\"\n",
    "  with tf.variable_scope(scope, 'vgg_19', [inputs]) as sc:\n",
    "    end_points_collection = sc.original_name_scope + '_end_points'\n",
    "    # Collect outputs for conv2d, fully_connected and max_pool2d.\n",
    "    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n",
    "                        outputs_collections=end_points_collection):\n",
    "      net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope='conv1')\n",
    "      net = slim.max_pool2d(net, [2, 2], scope='pool1')\n",
    "      net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope='conv2')\n",
    "      net = slim.max_pool2d(net, [2, 2], scope='pool2')\n",
    "      net = slim.repeat(net, 4, slim.conv2d, 256, [3, 3], scope='conv3')\n",
    "      net = slim.max_pool2d(net, [2, 2], scope='pool3')\n",
    "      net = slim.repeat(net, 4, slim.conv2d, 512, [3, 3], scope='conv4')\n",
    "      net = slim.max_pool2d(net, [2, 2], scope='pool4')\n",
    "      net = slim.repeat(net, 4, slim.conv2d, 512, [3, 3], scope='conv5')\n",
    "      net = slim.max_pool2d(net, [2, 2], scope='pool5')\n",
    "\n",
    "      # Use conv2d instead of fully_connected layers.\n",
    "      net = slim.conv2d(net, 4096, [7, 7], padding=fc_conv_padding, scope='fc6')\n",
    "      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n",
    "                         scope='dropout6')\n",
    "      net = slim.conv2d(net, 4096, [1, 1], scope='fc7')\n",
    "      # Convert end_points_collection into a end_point dict.\n",
    "      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n",
    "      if global_pool:\n",
    "        net = tf.reduce_mean(net, [1, 2], keep_dims=True, name='global_pool')\n",
    "        end_points['global_pool'] = net\n",
    "      if num_classes:\n",
    "        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n",
    "                           scope='dropout7')\n",
    "        net = slim.conv2d(net, num_classes, [1, 1],\n",
    "                          activation_fn=None,\n",
    "                          normalizer_fn=None,\n",
    "                          scope='fc8')\n",
    "        if spatial_squeeze:\n",
    "          net = tf.squeeze(net, [1, 2], name='fc8/squeezed')\n",
    "        end_points[sc.name + '/fc8'] = net\n",
    "      return net, end_points\n",
    "vgg_19.default_image_size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "net, end_points = vgg_19(inputs=tf.random_uniform((32, 224, 224, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'vgg_19/fc8/squeezed:0' shape=(32, 1000) dtype=float32>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'vgg_19/conv1/conv1_1/weights:0' shape=(3, 3, 3, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'vgg_19/conv1/conv1_1/biases:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'vgg_19/conv1/conv1_2/weights:0' shape=(3, 3, 64, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'vgg_19/conv1/conv1_2/biases:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'vgg_19/conv2/conv2_1/weights:0' shape=(3, 3, 64, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'vgg_19/conv2/conv2_1/biases:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'vgg_19/conv2/conv2_2/weights:0' shape=(3, 3, 128, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'vgg_19/conv2/conv2_2/biases:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'vgg_19/conv3/conv3_1/weights:0' shape=(3, 3, 128, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'vgg_19/conv3/conv3_1/biases:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'vgg_19/conv3/conv3_2/weights:0' shape=(3, 3, 256, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'vgg_19/conv3/conv3_2/biases:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'vgg_19/conv3/conv3_3/weights:0' shape=(3, 3, 256, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'vgg_19/conv3/conv3_3/biases:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'vgg_19/conv3/conv3_4/weights:0' shape=(3, 3, 256, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'vgg_19/conv3/conv3_4/biases:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'vgg_19/conv4/conv4_1/weights:0' shape=(3, 3, 256, 512) dtype=float32_ref>,\n",
       " <tf.Variable 'vgg_19/conv4/conv4_1/biases:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'vgg_19/conv4/conv4_2/weights:0' shape=(3, 3, 512, 512) dtype=float32_ref>,\n",
       " <tf.Variable 'vgg_19/conv4/conv4_2/biases:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'vgg_19/conv4/conv4_3/weights:0' shape=(3, 3, 512, 512) dtype=float32_ref>,\n",
       " <tf.Variable 'vgg_19/conv4/conv4_3/biases:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'vgg_19/conv4/conv4_4/weights:0' shape=(3, 3, 512, 512) dtype=float32_ref>,\n",
       " <tf.Variable 'vgg_19/conv4/conv4_4/biases:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'vgg_19/conv5/conv5_1/weights:0' shape=(3, 3, 512, 512) dtype=float32_ref>,\n",
       " <tf.Variable 'vgg_19/conv5/conv5_1/biases:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'vgg_19/conv5/conv5_2/weights:0' shape=(3, 3, 512, 512) dtype=float32_ref>,\n",
       " <tf.Variable 'vgg_19/conv5/conv5_2/biases:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'vgg_19/conv5/conv5_3/weights:0' shape=(3, 3, 512, 512) dtype=float32_ref>,\n",
       " <tf.Variable 'vgg_19/conv5/conv5_3/biases:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'vgg_19/conv5/conv5_4/weights:0' shape=(3, 3, 512, 512) dtype=float32_ref>,\n",
       " <tf.Variable 'vgg_19/conv5/conv5_4/biases:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'vgg_19/fc6/weights:0' shape=(7, 7, 512, 4096) dtype=float32_ref>,\n",
       " <tf.Variable 'vgg_19/fc6/biases:0' shape=(4096,) dtype=float32_ref>,\n",
       " <tf.Variable 'vgg_19/fc7/weights:0' shape=(1, 1, 4096, 4096) dtype=float32_ref>,\n",
       " <tf.Variable 'vgg_19/fc7/biases:0' shape=(4096,) dtype=float32_ref>,\n",
       " <tf.Variable 'vgg_19/fc8/weights:0' shape=(1, 1, 4096, 1000) dtype=float32_ref>,\n",
       " <tf.Variable 'vgg_19/fc8/biases:0' shape=(1000,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(tf.trainable_variables()))\n",
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./weight/vgg_19.ckpt\n"
     ]
    }
   ],
   "source": [
    "saver.restore(sess,'./weight/vgg_19.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_name_list = []\n",
    "for cache in tf.trainable_variables():\n",
    "    name = cache.name.replace('/','__').replace(':','')\n",
    "    all_name_list.append(name)\n",
    "    np.save('./weight/'+name, sess.run(cache))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./weight/all_name_list.txt','w') as flie:\n",
    "    for cache in all_name_list:\n",
    "        flie.writelines(cache + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.applications.vgg19.VGG19(include_top=False, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'block1_conv1/kernel:0' shape=(3, 3, 3, 64) dtype=float32>,\n",
       " <tf.Variable 'block1_conv1/bias:0' shape=(64,) dtype=float32>,\n",
       " <tf.Variable 'block1_conv2/kernel:0' shape=(3, 3, 64, 64) dtype=float32>,\n",
       " <tf.Variable 'block1_conv2/bias:0' shape=(64,) dtype=float32>,\n",
       " <tf.Variable 'block2_conv1/kernel:0' shape=(3, 3, 64, 128) dtype=float32>,\n",
       " <tf.Variable 'block2_conv1/bias:0' shape=(128,) dtype=float32>,\n",
       " <tf.Variable 'block2_conv2/kernel:0' shape=(3, 3, 128, 128) dtype=float32>,\n",
       " <tf.Variable 'block2_conv2/bias:0' shape=(128,) dtype=float32>,\n",
       " <tf.Variable 'block3_conv1/kernel:0' shape=(3, 3, 128, 256) dtype=float32>,\n",
       " <tf.Variable 'block3_conv1/bias:0' shape=(256,) dtype=float32>,\n",
       " <tf.Variable 'block3_conv2/kernel:0' shape=(3, 3, 256, 256) dtype=float32>,\n",
       " <tf.Variable 'block3_conv2/bias:0' shape=(256,) dtype=float32>,\n",
       " <tf.Variable 'block3_conv3/kernel:0' shape=(3, 3, 256, 256) dtype=float32>,\n",
       " <tf.Variable 'block3_conv3/bias:0' shape=(256,) dtype=float32>,\n",
       " <tf.Variable 'block3_conv4/kernel:0' shape=(3, 3, 256, 256) dtype=float32>,\n",
       " <tf.Variable 'block3_conv4/bias:0' shape=(256,) dtype=float32>,\n",
       " <tf.Variable 'block4_conv1/kernel:0' shape=(3, 3, 256, 512) dtype=float32>,\n",
       " <tf.Variable 'block4_conv1/bias:0' shape=(512,) dtype=float32>,\n",
       " <tf.Variable 'block4_conv2/kernel:0' shape=(3, 3, 512, 512) dtype=float32>,\n",
       " <tf.Variable 'block4_conv2/bias:0' shape=(512,) dtype=float32>,\n",
       " <tf.Variable 'block4_conv3/kernel:0' shape=(3, 3, 512, 512) dtype=float32>,\n",
       " <tf.Variable 'block4_conv3/bias:0' shape=(512,) dtype=float32>,\n",
       " <tf.Variable 'block4_conv4/kernel:0' shape=(3, 3, 512, 512) dtype=float32>,\n",
       " <tf.Variable 'block4_conv4/bias:0' shape=(512,) dtype=float32>,\n",
       " <tf.Variable 'block5_conv1/kernel:0' shape=(3, 3, 512, 512) dtype=float32>,\n",
       " <tf.Variable 'block5_conv1/bias:0' shape=(512,) dtype=float32>,\n",
       " <tf.Variable 'block5_conv2/kernel:0' shape=(3, 3, 512, 512) dtype=float32>,\n",
       " <tf.Variable 'block5_conv2/bias:0' shape=(512,) dtype=float32>,\n",
       " <tf.Variable 'block5_conv3/kernel:0' shape=(3, 3, 512, 512) dtype=float32>,\n",
       " <tf.Variable 'block5_conv3/bias:0' shape=(512,) dtype=float32>,\n",
       " <tf.Variable 'block5_conv4/kernel:0' shape=(3, 3, 512, 512) dtype=float32>,\n",
       " <tf.Variable 'block5_conv4/bias:0' shape=(512,) dtype=float32>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(model.trainable_weights))\n",
    "model.trainable_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 3, 3, 64),\n",
       " (64,),\n",
       " (3, 3, 64, 64),\n",
       " (64,),\n",
       " (3, 3, 64, 128),\n",
       " (128,),\n",
       " (3, 3, 128, 128),\n",
       " (128,),\n",
       " (3, 3, 128, 256),\n",
       " (256,),\n",
       " (3, 3, 256, 256),\n",
       " (256,),\n",
       " (3, 3, 256, 256),\n",
       " (256,),\n",
       " (3, 3, 256, 256),\n",
       " (256,),\n",
       " (3, 3, 256, 512),\n",
       " (512,),\n",
       " (3, 3, 512, 512),\n",
       " (512,),\n",
       " (3, 3, 512, 512),\n",
       " (512,),\n",
       " (3, 3, 512, 512),\n",
       " (512,),\n",
       " (3, 3, 512, 512),\n",
       " (512,),\n",
       " (3, 3, 512, 512),\n",
       " (512,),\n",
       " (3, 3, 512, 512),\n",
       " (512,),\n",
       " (3, 3, 512, 512),\n",
       " (512,)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[o.shape for o in cache]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, o in enumerate(cache):\n",
    "    np.save('./weight/' + f'keras_{i}', o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 经比对，https://github.com/tensorflow/models/tree/master/research/slim 和 keras官方的数据 --- VGG19的权重，完全一致。可以放心用来做风格迁移或别的事情。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 3, 128, 256),\n",
       " array([-0.00428896, -0.00857334,  0.00803106, -0.00418282, -0.00523086,\n",
       "         0.00288624,  0.00736083, -0.0009801 ,  0.00206848, -0.01747165,\n",
       "         0.01422966, -0.00294823, -0.00089182, -0.00555578,  0.04675391,\n",
       "         0.00404506, -0.00137131, -0.00535375,  0.00187276,  0.00498235],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "see1 = np.load(r'd:\\sync-cs\\bluoveGitHub\\note-on-ai\\image-style-transfer\\weight\\keras_8.npy')\n",
    "[see1.shape, see1[0,0,0,0:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 3, 128, 256),\n",
       " array([-0.00428896, -0.00857334,  0.00803106, -0.00418282, -0.00523086,\n",
       "         0.00288624,  0.00736083, -0.0009801 ,  0.00206848, -0.01747165,\n",
       "         0.01422966, -0.00294823, -0.00089182, -0.00555578,  0.04675391,\n",
       "         0.00404506, -0.00137131, -0.00535375,  0.00187276,  0.00498235],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "see2 = np.load(r'd:\\sync-cs\\bluoveGitHub\\note-on-ai\\image-style-transfer\\weight\\vgg_19__conv3__conv3_1__weights0.npy')\n",
    "[see2.shape, see2[0,0,0,0:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
